config: ''
run_name: auto
batch_size: 256
epochs: 100
total_tokens: -1
accum_iter: 1
save_ckpt_freq: 20
use_act_checkpoint: false
no_use_act_checkpoint: true
model: fm_base_12e_12d_swiglu_nobias
patch_size: 16
input_size: 224
num_register_tokens: 0
dtype: bfloat16
num_input_tokens: 128
num_target_tokens: 128
min_input_tokens: null
min_target_tokens: null
loss_type: mod
finetune: ''
opt: adamw
opt_eps: 1e-8
opt_betas: '[0.9'
clip_grad: null
momentum: 0.9
weight_decay: 0.05
weight_decay_end: null
skip_nan_grad: false
blr: 1e-4
min_blr: 0.0
frozen_model_blr: -1
scheduler: cosine
warmup_epochs: 10
warmup_steps: -1
warmup_tokens: -1
cooldown_epochs: 10
cooldown_steps: -1
cooldown_tokens: -1
frozen_model_epochs: 0
frozen_model_tokens: 0
frozen_embedding_domain: null
data_config: ''
epoch_size: null
s3_endpoint: ''
s3_data_endpoint: null
s3_multipart_chunksize_mb: 512
s3_multipart_threshold_mb: 512
s3_max_io_queue: 100
text_tokenizer_path: fourm/utils/tokenizer/trained/text_tokenizer_4m_wordpiece_30k.json
eval_freq: 10
dist_eval: false
no_dist_eval: true
fixed_eval: false
no_fixed_eval: true
fixed_eval_input_tokens: 128
fixed_eval_target_tokens: 128
fixed_eval_batch_size: 32
eval: false
output_dir: ''
device: cuda
seed: 0
resume: ''
auto_resume: false
no_auto_resume: true
start_epoch: 0
num_workers: 10
pin_mem: false
no_pin_mem: true
rlimit: 4096
print_all: false
s3_save_dir: ''
show_user_warnings: false
dist_url: env://
log_wandb: false
no_log_wandb: true
wandb_project: null
wandb_entity: null
wandb_run_name: auto
